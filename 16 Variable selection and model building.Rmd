---
title: "16 Varialbe Selection and Model Building"
author: "kwon"
date: "2018년 7월 5일"
output: html_document
---

# Chapter 16. Variable Selectoin and Model Building

## 1. Quality Criteria

$R^2$ and $RMSE$ improve with increasing the size of a model. This suggests that we need a quality criteria that takes into account the size of the model, since our preference is for small models that still fit well. Smaller model with "goodness-of-fit". In stattistics, we will look at three criteria: AIC, BIC, and adjusted $R^2$. \n

### 1.1 Akaike Infrosmation Criterion

Recall, the maximized log-likelihood of a regression model can be written as
$$
logL(\hat\beta, \hat\sigma^2) = -\frac{n}{2}log(2\pi)
-\frac{n}{2}log(\frac{RSS}{n})- \frac{n}{2}
$$

where $RSS = \sum_{i=1}^{n}(y_i-\hat y_i)^2$ and $\hat\beta$ and $\hat\sigma^2$ were chosen to maximize the likelihood
\n

Then we can define AIC as
$$
AIC = -2logL(\hat\beta, \hat\sigma^2) + 2- = n+ nlog(2\pi)+nlog(\frac{RSS}{n})+2p
$$
The likelihodd(which measures "goodness-of-fit") and the penalty are the two main componenets of AIC. Since other two components are constant, they can be eliminated.
$$
AIC = nlog(\frac{RSS}{n}) +2p
$$
\n
### 1.2 Bayesian Information Criterion
BIC quantifies the trade off betweena model which fits well and the number of model parameters, however for a reasonable sample size, generally picks a smaller model than AIC
$$
BIC = -2logL(\hat\beta, \hat\sigma^2) + log(n)p = n + nlog(2\[i]) + nlog(\frac{RSS}{n}) + log(n)p
$$
\n
### 1.3 Adjusted R-suared
$$
R^2 = 1 - \frac{SSE}{SST} = 1- \frac{\sum_{i=1}^2(y_i -\hat y_i)^2}{\sum_{i=1}^n(y_i - \bar y)^2}
$$
$$
R_a^2 = 1 - \frac{SSE/(n-p)}{SST/(n-1)} = 1 - (\frac{n-1}{n-p})(1-R^2)
$$
is called the Adjusted $R^2$

### 1.4 Cross-Validated RMSE

```{r}
make_poly_data = function(sample_size=11){
  x= seq(0,10)
  y = 3+x+4*x^2 + rnorm(n=sample_size, mean=0, sd=20)
  data.frame(x,y)
}

set.seed(1234)
poly_data =make_poly_data()
```

```{r}
fit_quad = lm(y~poly(x, degree=2), data=poly_data)
fit_big = lm(y~poly(x, degree=8), data=poly_data)

plot(y~x, data=poly_data, ylim = c(-100, 400), cex=2, pch=20)
xplot = seq(0, 10, by=0.1)
lines(xplot, predict(fit_quad, newdata = data.frame(x=xplot)),
      col= "dodgerblue", lwd=2, lty=1)
lines(xplot, predict(fit_big, newdata =data.frame(x = xplot)),
      col= "darkorange", lwd=2, lty=2)
```
The dahsed orange curve fits the points better, however it is unlikely that it is correctly modeling. This is an example of **overfitting**.

```{r}
sqrt(mean(resid(fit_quad)^2))
sqrt(mean(resid(fit_big)^2))
```
To correct for this, we will introduce cross-validation. leave-one-out cross validated RMSE to be
$$ RMSE_{LOOCV} = \sqrt{\frac{1}{n}\sum_{i=1}^ne_{[i]}^2} $$
The $e_{[i]}$ are the residual for the ith observation, when that observation is not used to fit the model. \n
In general, to perfor this calculation, we would be required to fit the model n times, once with each possible observation removed. However, for leave-one-out cross-validation and linear models, the equation can be rewritten as,
$$ RMSE_{LOOCV} = \sqrt{\frac{1}{n}\sum_{i=1}^n(\frac{e_i}{1-h_i})^2} $$
where h are the leverages and e are the usual residuals.

```{r}
calc_loocv_rmse = function(model){
  sqrt(mean((resid(model) / (1-hatvalues(model)))^2 ))
}
calc_loocv_rmse(fit_quad)
calc_loocv_rmse(fit_big)
```

```{r}
fit_quad_removed = lm(y~poly(x,degree=2), data= poly_data[-3,])
fit_big_removed = lm(y~poly(x,degree=8), data= poly_data[-3,])

plot(y ~ x, data = poly_data, ylim = c(-100, 400), cex = 2, pch = 20)
xplot = seq(0, 10, by = 0.1)
lines(xplot, predict(fit_quad_removed, newdata = data.frame(x = xplot)),
      col = "dodgerblue", lwd = 2, lty = 1)
lines(xplot, predict(fit_big_removed, newdata = data.frame(x = xplot)),
      col = "darkorange", lwd = 2, lty = 2)
```

## 2. Slection Procedures

```{r}
library(faraway)
hipcenter_mod = lm(hipcenter ~., data=seatpos)
coef(hipcenter_mod)
```
Let's consider only models with first order terms, thus no interactions and no polynomials. Then there are
$$  \sum_{k=0}^{p-1}\binom{p-1}{k} = 2^{p-1} = 2^8 $$

### 2.1 Backward Search

```{r}
hipcenter_mod_back_aic = step(hipcenter_mod, direction= "backward")
```
```{r}
extractAIC(hipcenter_mod)
n = length(resid(hipcenter_mod))
(p = length(coef(hipcenter_mod)))
n*log(mean(resid(hipcenter_mod)^2))+2*p
```

By specifying `k = log(n)`, we can get BIC.

```{r}
hipcenter_mod_back_bic = step(hipcenter_mod, direction = "backward", k=log(n))
```
We note that this model is smaller, has fewer predictors, than the model chosen by AIC.

```{r}
summary(hipcenter_mod)$adj.r.squared
summary(hipcenter_mod_back_aic)$adj.r.squared
summary(hipcenter_mod_back_bic)$adj.r.squared

calc_loocv_rmse(hipcenter_mod)
calc_loocv_rmse(hipcenter_mod_back_aic)
calc_loocv_rmse(hipcenter_mod_back_bic)
```

### 2.2 Forward Search
In forward selection, we tell `R` to start with a model without predictors, `hipcenter~1`, then at each step `R` will attempt to add a predictor untill it finds a good model or reaches full model.

```{r}
hipcenter_mod_start = lm(hipcenter~1, data=seatpos)
hipcenter_mod_forw_aic = step(
  hipcenter_mod_start,
  scope = hipcenter~Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg,
  direction="forward"
)
```

```{r}
hipcenter_mod_forw_bic = step(
  hipcenter_mod_start, 
  scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, 
  direction = "forward", k = log(n))
```

```{r}
summary(hipcenter_mod)$adj.r.squared
summary(hipcenter_mod_forw_aic)$adj.r.squared
summary(hipcenter_mod_forw_bic)$adj.r.squared

calc_loocv_rmse(hipcenter_mod)
calc_loocv_rmse(hipcenter_mod_forw_aic)
calc_loocv_rmse(hipcenter_mod_forw_bic)
```

### 2.3 Stepwise Search
Stepwise search checks the addition of any variables not currenty in the model, as well as the removal of any variable currenty in the model. \n
We start with the null model, search up to full model.
```{r}
hipcenter_mod_both_aic = step(
  hipcenter_mod_start, 
  scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, 
  direction = "both")
```
```{r}
hipcenter_mod_both_bic = step(
  hipcenter_mod_start, 
  scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, 
  direction = "both", k = log(n))
```

```{r}
summary(hipcenter_mod)$adj.r.squared
summary(hipcenter_mod_both_aic)$adj.r.squared
summary(hipcenter_mod_both_bic)$adj.r.squared

calc_loocv_rmse(hipcenter_mod)
calc_loocv_rmse(hipcenter_mod_back_aic)
calc_loocv_rmse(hipcenter_mod_back_bic)
```

### 2.4 Exhaustive Search
With a reasonably sized datset, it is not too difficult to check all possible models. To do so, we will use the `regsubsets()` function in `R` packages `leaps`.

```{r}
library(leaps)
all_hipcenter_mod = summary(regsubsets(hipcenter~., data=seatpos))
```
`regsubsets()` checks all possible subsets out of model `hipcenter~.`.

```{r}
all_hipcenter_mod$which
```

```{r}
all_hipcenter_mod$adjr2
(best_r2_ind = which.max(all_hipcenter_mod$adjr2))
```
```{r}
all_hipcenter_mod$which[best_r2_ind, ]
p = length(coef(hipcenter_mod))
n = length(resid(hipcenter_mod))
```
```{r}
hipcenter_mod_aic = n*log(all_hipcenter_mod$rss/n)+2*(2:p)
best_aic_ind = which.min(hipcenter_mod_aic)
all_hipcenter_mod$which[best_aic_ind,]
```
```{r}
hipcenter_mod_best_aic = lm(hipcenter ~ Age + Ht + Leg, data = seatpos)
extractAIC(hipcenter_mod_best_aic)
extractAIC(hipcenter_mod_back_aic)
extractAIC(hipcenter_mod_forw_aic)
extractAIC(hipcenter_mod_both_aic)
```

```{r}
plot(hipcenter_mod_aic~I(2:p),  ylab = "AIC", xlab = "p, number of parameters", 
     pch = 20, col = "dodgerblue", type = "b", cex = 2,
     main = "AIC vs Model Complexity")
```
Repeat this process for BIC
```{r}
hipcenter_mod_bic =n*log(all_hipcenter_mod$rss/n)+log(n)*(2:p)
best_bic_ind = which.min(hipcenter_mod_bic)
all_hipcenter_mod$which[best_bic_ind,]

hipcenter_mod_best_bic = lm(hipcenter ~ Ht, data = seatpos)
extractAIC(hipcenter_mod_best_bic, k =log(n))
extractAIC(hipcenter_mod_back_bic, k =log(n))
extractAIC(hipcenter_mod_forw_bic, k =log(n))
extractAIC(hipcenter_mod_both_bic, k =log(n))
```

## 3. Higher Order Terms

```{r}
autompg = read.table(
  "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data",
  quote = "\"",
  comment.char = "",
  stringsAsFactors = FALSE)
colnames(autompg) = c("mpg", "cyl", "disp", "hp", "wt", "acc",
                      "year", "origin", "name")
autompg = subset(autompg, autompg$hp != "?")
autompg = subset(autompg, autompg$name != "plymouth reliant")
rownames(autompg) = paste(autompg$cyl, "cylinder", autompg$year, autompg$name)
autompg$hp = as.numeric(autompg$hp)
autompg$domestic= as.numeric(autompg$origin==1)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]
autompg$cyl = as.factor(autompg$cyl)
autompg$domestic = as.factor(autompg$domestic)
autompg = subset(autompg, select = c("mpg", "cyl", "disp", "hp", 
                                     "wt", "acc", "year", "domestic"))
```
```{r}
str(autompg)
```
```{r}
pairs(autompg, col="dodgerblue")
```
`pairs` plot to determine which variables may benefit from quadratic relationship with the response.

```{r}
autompg_big_mod = lm(
  log(mpg)~.^2 + I(disp^2) + I(hp^2) + I(wt^2)+I(acc^2),
  data = autompg
)
length(coef(autompg_big_mod))

autompg_mod_back_aic = step(autompg_big_mod, direction = "backward", trace=0)
```
`trace=0` in the function call suppress the output for each step, and simply stores the chosen model.

```{r}
n = length(resid(autompg_big_mod))
autompg_mod_back_bic = step(autompg_big_mod, direction ="backward", k =log(n), trace=0)
```

```{r}
coef(autompg_mod_back_aic)
coef(autompg_mod_back_bic)
length(coef(autompg_big_mod))
length(coef(autompg_mod_back_aic))
length(coef(autompg_mod_back_bic))
```
```{r}
calc_loocv_rmse(autompg_big_mod)
calc_loocv_rmse(autompg_mod_back_aic)
calc_loocv_rmse(autompg_mod_back_bic)
```

## 4. Explanation versus Prediction

### 4.1 Explanation
In this case, we are intereseted in keeping models as small as possible, since smaller models are easy to interpret. Linear models are rather interpretable to begin with. \n
To find small and interpretable models, we would use selection criterion that explicitly penalize larger models, such as AIC and BIC.

#### 4.1.1 Correlation anc Causation

Correlation is often also referred to as association.
```{r}
plot(mpg~hp, data = autompg, col="dodgerblue", pch= 20, cex=1.5)
```

The data we have is observational data. It only detects associations. To speak with confidence about causality, we would need to run experiments.

### 4.2 Prediction

We would use selection criterion that implictly penalize larger models, such as **LOOC RMSE**. So long as the model does not over-fit, we do not actually care how large the model becomes.





