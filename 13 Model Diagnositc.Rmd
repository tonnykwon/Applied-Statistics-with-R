<<<<<<< HEAD
---
title: "13 Model Diagnositc"
author: "kwon"
date: "2018년 6월 27일"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Model Assumptions
Assumptions of Linear Regression \
Linearity: the response can be written as a linear combinations of the predictors. \
Independence: the errors are independent.\
Normality: the distribution of the errors should follow a normal distribution. \
Equal Variance: the error variance is the same at any set of predictor values.

# 2. Checking Assumptions

$$ Model 1: Y = 3+5x +\epsilon, \epsilon \sim \mathcal{N(0,1)}$$
$$ Model 2: Y = 3+5x +\epsilon, \epsilon \sim \mathcal{N(0,x^2)}$$
$$ Model 3: Y = 3+5x^2 +\epsilon, \epsilon \sim \mathcal{N(0,25)}$$

```{r hello}
sim_1 = function(sample_size = 500){
  x= runif(n=sample_size)*5
  y = 3+5*x + rnorm(n=sample_size, mean=0, sd=1)
  data.frame(x,y)
}

sim_2 = function(sample_size = 500){
  x = runif(n=sample_size)*5
  y = 3+5*x + rnorm(n=sample_size, mean=0, sd=x)
  data.frame(x,y)
}

sim_3 = function(sample_size =500){
  x = runif(n=sample_size)*5
  y = 3 + 5*x^2 +rnorm(n=sample_size, mean=0, sd=5)
  data.frame(x,y)
}

```

##2.1 Fitted versus Residual Plots

Useful for checking the linearity and constant variance
```{r}
set.seed(42)
sim_data_1 = sim_1()
head(sim_data_1)
```
```{r}
plot(y~x, data=sim_data_1, col="grey", pch=20, main= "Data From Model 1")
fit_1 = lm(y~x, data=sim_data_1)
abline(fit_1, col= "darkorange", lwd=3)
```
```{r}
plot(fitted(fit_1), resid(fit_1), pch=20, col="grey", main = "Fitted Versus Residual", xlab = "Fitted", ylab="Residual")
abline(h =0, col="darkorange", lwd=2)
```

```{r}
set.seed(42)
sim_data_2 = sim_2()
fit_2 = lm(y~x, data = sim_data_2)
plot(y ~ x, data= sim_data_2, col="grey", pch=20, main = "Data frm Model 2")
abline(fit_2, col="darkorange", lwd=3)

set.seed(42)
sim_data_3 = sim_3()
fit_3 = lm(y~x, data = sim_data_3)
plot(y ~ x, data= sim_data_3, col="grey", pch=20, main = "Data frm Model 3")
abline(fit_3, col="darkorange", lwd=3)
```
```{r}
plot(fitted(fit_2), resid(fit_2), col="grey", pch=20, xlab ="Fitted", ylab="Residuals", main = "Fitted Versus Residual")
abline(h=0, col="darkorange", lwd=2)

plot(fitted(fit_3), resid(fit_3), col="grey", pch=20, xlab ="Fitted", ylab="Residuals", main = "Fitted Versus Residual")
abline(h=0, col="darkorange", lwd=2)
```

Model 2: Constant variance assumption is violated
Model 3: Linearity assumption is violated


## 2.2 Breusch-Pagan Test
H0: Homoscedasticity. The errors have constant variance about the true model. \
H1: Heteroscedasticity. The errors have non-constant variance about the true model.

```{r}
library(lmtest)
bptest(fit_1)
bptest(fit_2)
bptest(fit_3)
```

## 2.3 Historgram
```{r}
par(mfrow = c(1, 3))
hist(resid(fit_1),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_1",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_2),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_2",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_3),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_3",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
```
## 2.4 Q-Q Plots
Normal Quantile-Quantile plot
```{r}
# qqnorm() plots the points
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
# qqline adds the necessary line
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)
```
```{r}
qq_plot = function(e){
  n = length(e)
  normal_quantiles = qnorm(((1:n-0.5)/n))
  
  # plot theoretical versus observed quantiles
  plot(normal_quantiles, sort(e),
       xlab = c("Theoretical Quantiles"),
       ylab = c("Sample Quantiles"),
       col = "darkgrey")
  title("Normal Q-Q Plot")
  
  # calculate line through the first and third quantiles
  slope = (quantile(e, 0.75) - quantile(e, 0.25)) / (qnorm(0.75)-qnorm(0.25))
  intercept = quantile(e, 0.25) - slope*qnorm(0.25)
  
  # add to existing plot
  abline(intercept, slope, lty=2, lwd=2, col="dodgerblue")
}

set.seed(420)
x = rnorm(100, mean = 0 , sd = 1)
par(mfrow = c(1, 2))
qqnorm(x, col = "darkgrey")
qqline(x, lty = 2, lwd = 2, col = "dodgerblue")
qq_plot(x)
```
```{r}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rnorm(10))
qq_plot(rnorm(25))
qq_plot(rnorm(100))
```
```{r}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rt(10, df = 4))
qq_plot(rt(25, df = 4))
qq_plot(rt(100, df = 4))
```
```{r}
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_2), main = "Normal Q-Q Plot, fit_2", col = "darkgrey")
qqline(resid(fit_2), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_3), main = "Normal Q-Q Plot, fit_3", col = "darkgrey")
qqline(resid(fit_3), col = "dodgerblue", lwd = 2)
```

## 2.5 Shapiro-Wilk Test
```{r}
set.seed(42)
shapiro.test(rnorm(25))

shapiro.test(rexp(25))
```
```{r}
shapiro.test(resid(fit_1))
shapiro.test(resid(fit_2))
shapiro.test(resid(fit_3))
```

# 3. Unsusual Observations

```{r}
par(mforw= c(1,3))
set.seed(42)
ex_data = data.frame(x=1:10, y = 10:1 + rnorm(n=10))
ex_model = lm(y~x, data= ex_data)

# Low leverage, large residual, small influence
point_1 = c(5.4, 11)
ex_data_1 = rbind(ex_data, point_1)
model_1 = lm(y ~ x, data = ex_data_1)
plot(y~x, data= ex_data_1, cex= 2, pch = 20, col="grey", main="Low leverage, Large residual, Small influence")
points(x=point_1[1], y=point_1[2], pch=1, cex=4, col="black", lwd=2)
abline(ex_model, col="dodgerblue", lwd=2)
abline(model_1, lty = 2, col="darkorange", lwd=2)
legend("bottomleft", c("Original Data", "Added Point"), lty=c(1,2), col=c("dodgerblue", "darkorange"))

# high leverage, small residual, small influence
point_2 = c(18, -5.7)
ex_data_2 = rbind(ex_data, point_2)
model_2 = lm(y ~ x, data = ex_data_2)
plot(y ~ x, data = ex_data_2, cex = 2, pch = 20, col = "grey",
     main = "High Leverage, Small Residual, Small Influence")
points(x = point_2[1], y = point_2[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_2, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
       lty = c(1, 2), col = c("dodgerblue", "darkorange"))

# high leverage, large residual, large influence
point_3 = c(14, 5.1)
ex_data_3 = rbind(ex_data, point_3)
model_3 = lm(y ~ x, data = ex_data_3)
plot(y ~ x, data = ex_data_3, cex = 2, pch = 20, col = "grey", ylim = c(-3, 12),
     main = "High Leverage, Large Residual, Large Influence")
points(x = point_3[1], y = point_3[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_3, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
       lty = c(1, 2), col = c("dodgerblue", "darkorange"))
```
```{r}
coef(ex_model)[2]
coef(model_1)[2]
coef(model_2)[2]
coef(model_3)[2]
```

## 3.1 Leverage
A data point with high leverage, is a data point that could have large influence when fitting the model.

$$ \hat\beta = (X^TX)^{-1}X^Ty $$
$$ \hat{y} = \hat\beta = X(X^TX)^{-1}X^Ty $$
$$ H = X(X^TX)^{-1}X^T $$
Hat matrix is used to project onto the subspace spanned by the columns of X.
$$ \hat y = Hy $$
The diagnoal elements of this matrix are called leverages
$$ H_{ii} = h_{i} $$
```{r}
lev_ex = data.frame(
  x1 = c(0, 11, 11, 7, 4, 10, 5, 8),
  x2 = c(1, 5, 4, 3, 1, 4, 4, 2),
  y = c(11, 15, 13, 14, 0, 19, 16, 8)
)
plot(x2 ~ x1, data=lev_ex, cex=2)
points(7,3, pch=20, col ="red", cex=2)
```
```{r}
X = cbind(rep(1,8), lev_ex$x1, lev_ex$x2)
H = X %*% solve(t(X)%*%X) %*% t(X)
diag(H)
sum(diag(H))
```
```{r}
lev_fit = lm(y~., data = lev_ex)
hatvalues(lev_fit)
coef(lev_fit)
```
```{r}
lev_ex[which.max(hatvalues(lev_fit)),]
lev_ex_1 = lev_ex
lev_ex_1$y[1] =20
lm(y~., data=lev_ex_1)
```
```{r}
lev_ex[which.min(hatvalues(lev_fit)),]
lev_ex_2 = lev_ex
lev_ex_2$y[4] = 30
lm(y ~ ., data = lev_ex_2)
```
Changing the value of y which has the highest hat value causes large changes in the coefficients, while changing that of y with the smallest hat value changes intercept little

```{r}
hatvalues(model_1) > 2 * mean(hatvalues(model_1))
hatvalues(model_2) > 2 * mean(hatvalues(model_2))
hatvalues(model_3) > 2 * mean(hatvalues(model_3))
```

## 3.2 Outliers
Outliers are points which do not fit the model well.
$$ e= y-\hat y = Iy - Hy = (I-H)y $$

$$ Var(e_{i}) = (1-h_i)\sigma^2) $$
$$ SE[e_i] = s_e\sqrt{(1-h_i)}$$
Standardized residual when n is large
$$ r_i = \frac{e_i}{s_e\sqrt{1-h_i}} \sim N(\mu = 0, \sigma^2 = 1) $$

```{r}
resid(model_1)
rstandard(model_1)
```
```{r}
rstandard(model_1)[abs(rstandard(model_1))>2]
rstandard(model_2)[abs(rstandard(model_2))>2]
rstandard(model_3)[abs(rstandard(model_3))>2]
```
First plot has 11th point, the added point, with a large standardized residual. Second plot does not have one. \
The added point in plots two and three were both high leverage, but now only the point in plot three ahs a large residual.

## 3.3 Influence

Some outliers only change the regression a small amount(plot one) and some outliers have a large effect on the regression(plot three). Points with high leverage and large residual, we will call influential. \
\
A common measure of influence is **Cook's Distance**,
$$ D_i = \frac{1}{p}r_i^2 \frac{h_i}{1-h_i}$$

A Cook's Distance is often considered large, and called influential, if
$$ D_i > \frac{4}{h}$$

Plot One: low leverage, large residual. \
Plot Two: high leverage, small residual. \
Plot Three: high leverage, large residual.

```{r}
n = length(resid(model_1))
cooks.distance(model_1)[11] > 4 /n
cooks.distance(model_2)[11] > 4 /n
cooks.distance(model_3)[11] > 4 /n
```


# 4. Data Analysis Examples
## 4.1 Good Diagnositcs

```{r}
mpg_hp_add = lm(mpg~hp +am, data=mtcars)

plot(fitted(mpg_hp_add), resid(mpg_hp_add), col="grey", pch=20, xlab="Fitted", ylab="Residual", main = "matcars: Fitted Versus Residuals")
abline(h = 0, col = "darkorange", lwd=2)
```
```{r}
bptest(mpg_hp_add)

qqnorm(resid(mpg_hp_add), col= "darkgrey")
qqline(resid(mpg_hp_add), col="dodgerblue", lwd=2)
```
```{r}
shapiro.test(resid(mpg_hp_add))
```


```{r}
# Large leverage
sum(hatvalues(mpg_hp_add) > 2*mean(hatvalues(mpg_hp_add)))

# Large residual
sum(abs(rstandard(mpg_hp_add)) > 2)
```
```{r}
cd_mpg_hp_add = cooks.distance(mpg_hp_add)
sum(cd_mpg_hp_add > 4 / length(cd_mpg_hp_add))

large_cd_mpg = cd_mpg_hp_add > 4 / length(cd_mpg_hp_add)
cd_mpg_hp_add[large_cd_mpg]
```
We find two influential points, and they are very different cars.

```{r}
coef(mpg_hp_add)

mpg_hp_add_fix = lm(mpg~ hp+am, data=mtcars, subset = cd_mpg_hp_add<= 4/length(cd_mpg_hp_add))
coef(mpg_hp_add_fix)

```

```{r}
par(mfrow= c(2,2))
plot(mpg_hp_add)
```

## 4.2 Suspect Diagnostics
```{r}
setwd("C:/Users/tony/Documents/MCS/STAT 420/week8 13, 14, 17")
autompg = read.csv("auto-mpg.csv")
str(autompg)
colnames(autompg)[-1]<-c("cyl", "disp","hp","wt","acc","year", "origin", "domestic")
names(autompg)
big_model = lm(mpg~ disp*hp*domestic, data= autompg)
qqnorm(resid(big_model), col= "darkgrey")
qqline(resid(big_model), col = "dodgerblue", lwd=2)
```










=======
---
title: "13 Model Diagnositc"
author: "kwon"
date: "2018년 6월 27일"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Model Assumptions
Assumptions of Linear Regression \
Linearity: the response can be written as a linear combinations of the predictors. \
Independence: the errors are independent.\
Normality: the distribution of the errors should follow a normal distribution. \
Equal Variance: the error variance is the same at any set of predictor values.

# 2. Checking Assumptions

$$ Model 1: Y = 3+5x +\epsilon, \epsilon \sim \mathcal{N(0,1)}$$
$$ Model 2: Y = 3+5x +\epsilon, \epsilon \sim \mathcal{N(0,x^2)}$$
$$ Model 3: Y = 3+5x^2 +\epsilon, \epsilon \sim \mathcal{N(0,25)}$$

```{r hello}
sim_1 = function(sample_size = 500){
  x= runif(n=sample_size)*5
  y = 3+5*x + rnorm(n=sample_size, mean=0, sd=1)
  data.frame(x,y)
}

sim_2 = function(sample_size = 500){
  x = runif(n=sample_size)*5
  y = 3+5*x + rnorm(n=sample_size, mean=0, sd=x)
  data.frame(x,y)
}

sim_3 = function(sample_size =500){
  x = runif(n=sample_size)*5
  y = 3 + 5*x^2 +rnorm(n=sample_size, mean=0, sd=5)
  data.frame(x,y)
}

```

##2.1 Fitted versus Residual Plots

Useful for checking the linearity and constant variance
```{r}
set.seed(42)
sim_data_1 = sim_1()
head(sim_data_1)
```
```{r}
plot(y~x, data=sim_data_1, col="grey", pch=20, main= "Data From Model 1")
fit_1 = lm(y~x, data=sim_data_1)
abline(fit_1, col= "darkorange", lwd=3)
```
```{r}
plot(fitted(fit_1), resid(fit_1), pch=20, col="grey", main = "Fitted Versus Residual", xlab = "Fitted", ylab="Residual")
abline(h =0, col="darkorange", lwd=2)
```

```{r}
set.seed(42)
sim_data_2 = sim_2()
fit_2 = lm(y~x, data = sim_data_2)
plot(y ~ x, data= sim_data_2, col="grey", pch=20, main = "Data frm Model 2")
abline(fit_2, col="darkorange", lwd=3)

set.seed(42)
sim_data_3 = sim_3()
fit_3 = lm(y~x, data = sim_data_3)
plot(y ~ x, data= sim_data_3, col="grey", pch=20, main = "Data frm Model 3")
abline(fit_3, col="darkorange", lwd=3)
```
```{r}
plot(fitted(fit_2), resid(fit_2), col="grey", pch=20, xlab ="Fitted", ylab="Residuals", main = "Fitted Versus Residual")
abline(h=0, col="darkorange", lwd=2)

plot(fitted(fit_3), resid(fit_3), col="grey", pch=20, xlab ="Fitted", ylab="Residuals", main = "Fitted Versus Residual")
abline(h=0, col="darkorange", lwd=2)
```

Model 2: Constant variance assumption is violated
Model 3: Linearity assumption is violated


## 2.2 Breusch-Pagan Test
H0: Homoscedasticity. The errors have constant variance about the true model. \
H1: Heteroscedasticity. The errors have non-constant variance about the true model.

```{r}
library(lmtest)
bptest(fit_1)
bptest(fit_2)
bptest(fit_3)
```

## 2.3 Historgram
```{r}
par(mfrow = c(1, 3))
hist(resid(fit_1),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_1",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_2),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_2",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_3),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_3",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
```
## 2.4 Q-Q Plots
Normal Quantile-Quantile plot
```{r}
# qqnorm() plots the points
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
# qqline adds the necessary line
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)
```
```{r}
qq_plot = function(e){
  n = length(e)
  normal_quantiles = qnorm(((1:n-0.5)/n))
  
  # plot theoretical versus observed quantiles
  plot(normal_quantiles, sort(e),
       xlab = c("Theoretical Quantiles"),
       ylab = c("Sample Quantiles"),
       col = "darkgrey")
  title("Normal Q-Q Plot")
  
  # calculate line through the first and third quantiles
  slope = (quantile(e, 0.75) - quantile(e, 0.25)) / (qnorm(0.75)-qnorm(0.25))
  intercept = quantile(e, 0.25) - slope*qnorm(0.25)
  
  # add to existing plot
  abline(intercept, slope, lty=2, lwd=2, col="dodgerblue")
}

set.seed(420)
x = rnorm(100, mean = 0 , sd = 1)
par(mfrow = c(1, 2))
qqnorm(x, col = "darkgrey")
qqline(x, lty = 2, lwd = 2, col = "dodgerblue")
qq_plot(x)
```
```{r}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rnorm(10))
qq_plot(rnorm(25))
qq_plot(rnorm(100))
```
```{r}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rt(10, df = 4))
qq_plot(rt(25, df = 4))
qq_plot(rt(100, df = 4))
```
```{r}
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_2), main = "Normal Q-Q Plot, fit_2", col = "darkgrey")
qqline(resid(fit_2), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_3), main = "Normal Q-Q Plot, fit_3", col = "darkgrey")
qqline(resid(fit_3), col = "dodgerblue", lwd = 2)
```

## 2.5 Shapiro-Wilk Test
```{r}
set.seed(42)
shapiro.test(rnorm(25))

shapiro.test(rexp(25))
```
```{r}
shapiro.test(resid(fit_1))
shapiro.test(resid(fit_2))
shapiro.test(resid(fit_3))
```

# 3. Unsusual Observations

```{r}
par(mforw= c(1,3))
set.seed(42)
ex_data = data.frame(x=1:10, y = 10:1 + rnorm(n=10))
ex_model = lm(y~x, data= ex_data)

# Low leverage, large residual, small influence
point_1 = c(5.4, 11)
ex_data_1 = rbind(ex_data, point_1)
model_1 = lm(y ~ x, data = ex_data_1)
plot(y~x, data= ex_data_1, cex= 2, pch = 20, col="grey", main="Low leverage, Large residual, Small influence")
points(x=point_1[1], y=point_1[2], pch=1, cex=4, col="black", lwd=2)
abline(ex_model, col="dodgerblue", lwd=2)
abline(model_1, lty = 2, col="darkorange", lwd=2)
legend("bottomleft", c("Original Data", "Added Point"), lty=c(1,2), col=c("dodgerblue", "darkorange"))

# high leverage, small residual, small influence
point_2 = c(18, -5.7)
ex_data_2 = rbind(ex_data, point_2)
model_2 = lm(y ~ x, data = ex_data_2)
plot(y ~ x, data = ex_data_2, cex = 2, pch = 20, col = "grey",
     main = "High Leverage, Small Residual, Small Influence")
points(x = point_2[1], y = point_2[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_2, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
       lty = c(1, 2), col = c("dodgerblue", "darkorange"))

# high leverage, large residual, large influence
point_3 = c(14, 5.1)
ex_data_3 = rbind(ex_data, point_3)
model_3 = lm(y ~ x, data = ex_data_3)
plot(y ~ x, data = ex_data_3, cex = 2, pch = 20, col = "grey", ylim = c(-3, 12),
     main = "High Leverage, Large Residual, Large Influence")
points(x = point_3[1], y = point_3[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_3, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
       lty = c(1, 2), col = c("dodgerblue", "darkorange"))
```
```{r}
coef(ex_model)[2]
coef(model_1)[2]
coef(model_2)[2]
coef(model_3)[2]
```

## 3.1 Leverage
A data point with high leverage, is a data point that could have large influence when fitting the model.

$$ \hat\beta = (X^TX)^{-1}X^Ty $$
$$ \hat{y} = \hat\beta = X(X^TX)^{-1}X^Ty $$
$$ H = X(X^TX)^{-1}X^T $$
Hat matrix is used to project onto the subspace spanned by the columns of X.
$$ \hat y = Hy $$
The diagnoal elements of this matrix are called leverages
$$ H_{ii} = h_{i} $$
```{r}
lev_ex = data.frame(
  x1 = c(0, 11, 11, 7, 4, 10, 5, 8),
  x2 = c(1, 5, 4, 3, 1, 4, 4, 2),
  y = c(11, 15, 13, 14, 0, 19, 16, 8)
)
plot(x2 ~ x1, data=lev_ex, cex=2)
points(7,3, pch=20, col ="red", cex=2)
```
```{r}
X = cbind(rep(1,8), lev_ex$x1, lev_ex$x2)
H = X %*% solve(t(X)%*%X) %*% t(X)
diag(H)
sum(diag(H))
```
```{r}
lev_fit = lm(y~., data = lev_ex)
hatvalues(lev_fit)
coef(lev_fit)
```
```{r}
lev_ex[which.max(hatvalues(lev_fit)),]
lev_ex_1 = lev_ex
lev_ex_1$y[1] =20
lm(y~., data=lev_ex_1)
```
```{r}
lev_ex[which.min(hatvalues(lev_fit)),]
lev_ex_2 = lev_ex
lev_ex_2$y[4] = 30
lm(y ~ ., data = lev_ex_2)
```
Changing the value of y which has the highest hat value causes large changes in the coefficients, while changing that of y with the smallest hat value changes intercept little

```{r}
hatvalues(model_1) > 2 * mean(hatvalues(model_1))
hatvalues(model_2) > 2 * mean(hatvalues(model_2))
hatvalues(model_3) > 2 * mean(hatvalues(model_3))
```

## 3.2 Outliers
Outliers are points which do not fit the model well.
$$ e= y-\hat y = Iy - Hy = (I-H)y $$

$$ Var(e_{i}) = (1-h_i)\sigma^2) $$
$$ SE[e_i] = s_e\sqrt{(1-h_i)}$$
Standardized residual when n is large
$$ r_i = \frac{e_i}{s_e\sqrt{1-h_i}} \sim N(\mu = 0, \sigma^2 = 1) $$

```{r}
resid(model_1)
rstandard(model_1)
```
```{r}
rstandard(model_1)[abs(rstandard(model_1))>2]
rstandard(model_2)[abs(rstandard(model_2))>2]
rstandard(model_3)[abs(rstandard(model_3))>2]
```
First plot has 11th point, the added point, with a large standardized residual. Second plot does not have one. \
The added point in plots two and three were both high leverage, but now only the point in plot three ahs a large residual.

## 3.3 Influence

Some outliers only change the regression a small amount(plot one) and some outliers have a large effect on the regression(plot three). Points with high leverage and large residual, we will call influential. \
\
A common measure of influence is **Cook's Distance**,
$$ D_i = \frac{1}{p}r_i^2 \frac{h_i}{1-h_i}$$

A Cook's Distance is often considered large, and called influential, if
$$ D_i > \frac{4}{h}$$

Plot One: low leverage, large residual. \
Plot Two: high leverage, small residual. \
Plot Three: high leverage, large residual.

```{r}
n = length(resid(model_1))
cooks.distance(model_1)[11] > 4 /n
cooks.distance(model_2)[11] > 4 /n
cooks.distance(model_3)[11] > 4 /n
```


# 4. Data Analysis Examples
## 4.1 Good Diagnositcs

```{r}
mpg_hp_add = lm(mpg~hp +am, data=mtcars)

plot(fitted(mpg_hp_add), resid(mpg_hp_add), col="grey", pch=20, xlab="Fitted", ylab="Residual", main = "matcars: Fitted Versus Residuals")
abline(h = 0, col = "darkorange", lwd=2)
```
```{r}
bptest(mpg_hp_add)

qqnorm(resid(mpg_hp_add), col= "darkgrey")
qqline(resid(mpg_hp_add), col="dodgerblue", lwd=2)
```
```{r}
shapiro.test(resid(mpg_hp_add))
```


```{r}
# Large leverage
sum(hatvalues(mpg_hp_add) > 2*mean(hatvalues(mpg_hp_add)))

# Large residual
sum(abs(rstandard(mpg_hp_add)) > 2)
```
```{r}
cd_mpg_hp_add = cooks.distance(mpg_hp_add)
sum(cd_mpg_hp_add > 4 / length(cd_mpg_hp_add))

large_cd_mpg = cd_mpg_hp_add > 4 / length(cd_mpg_hp_add)
cd_mpg_hp_add[large_cd_mpg]
```
We find two influential points, and they are very different cars.

```{r}
coef(mpg_hp_add)

mpg_hp_add_fix = lm(mpg~ hp+am, data=mtcars, subset = cd_mpg_hp_add<= 4/length(cd_mpg_hp_add))
coef(mpg_hp_add_fix)

```

```{r}
par(mfrow= c(2,2))
plot(mpg_hp_add)
```

## 4.2 Suspect Diagnostics
```{r}
setwd("C:/Users/tony/Documents/MCS/STAT 420/week8 13, 14, 17")
autompg = read.csv("auto-mpg.csv")
str(autompg)
colnames(autompg)[-1]<-c("cyl", "disp","hp","wt","acc","year", "origin", "domestic")
names(autompg)
big_model = lm(mpg~ disp*hp*domestic, data= autompg)
qqnorm(resid(big_model), col= "darkgrey")
qqline(resid(big_model), col = "dodgerblue", lwd=2)
```










>>>>>>> 00a755ff5bce001ceda04215194ef606c1c71fc5
